{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a997142-cb5b-4864-b779-6106331ef69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common functions etc\n",
    "import re\n",
    "from calendar import isleap\n",
    "from datetime import datetime, date, timedelta\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Polygon, PathPatch, Patch, Rectangle, Circle\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.colors import to_hex\n",
    "from matplotlib.transforms import Bbox\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.colors import to_hex\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import colorsys\n",
    "import csv\n",
    "import math\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "def numeric_date(dt_string):\n",
    "    dt = datetime.strptime(dt_string, \"%Y-%m-%d\")\n",
    "    days_in_year = 366 if isleap(dt.year) else 365\n",
    "    return dt.year + (dt.timetuple().tm_yday-0.5) / days_in_year\n",
    "\n",
    "def summarise_dates(df):\n",
    "    \"\"\"just for my own curiosity / understanding of pandas\"\"\"\n",
    "    vals = list(df['date'])\n",
    "    # print(f\"n: {len(vals)}\")\n",
    "    for idx, v in enumerate(vals):\n",
    "        if not idx: continue\n",
    "        prev = vals[idx-1]\n",
    "        should_be = (date.fromisoformat(prev) + timedelta(days=1)).isoformat()\n",
    "        if should_be != v:\n",
    "            print(f\"Jumped from {prev} to {v} (should be {should_be})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd621b-19f1-46f0-9ffc-2b5f391018b3",
   "metadata": {},
   "source": [
    "## To do list\n",
    "\n",
    "* This requires hardcoded commit SHAs which are then fetched from GitHub. This isn't ideal (but it's easy). We should instead use something like `git log --oneline -- estimates/omicron-countries-split/omicron-countries-split_freq-combined-forecast-GARW.tsv` combined with `git show 4fec4d2:estimates/omicron-countries-split/omicron-countries-split_freq-combined-GARW.tsv` and then read that as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970e65a-1fdd-4588-8288-e42a0b20aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the latest model. This is thought of as the source of truth.\n",
    "model_latest = pd.read_table(\"https://raw.githubusercontent.com/blab/rt-from-frequency-dynamics/master/estimates/omicron-countries-split/omicron-countries-split_freq-combined-GARW.tsv\", sep=\"\\t\")\n",
    "model_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc999cf-69f2-446e-b0f1-f9970cf7a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest sequence counts (which will be converted to frequencies)\n",
    "seq_counts = pd.read_table(\"https://raw.githubusercontent.com/blab/rt-from-frequency-dynamics/master/data/omicron-countries-split/omicron-countries-split_location-variant-sequence-counts.tsv\", sep=\"\\t\")\n",
    "seq_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f908aa3-acf1-4759-a2bd-51b8cea78a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commits which modified the forecast TSV: https://github.com/blab/rt-from-frequency-dynamics/commits/master/estimates/omicron-countries-split/omicron-countries-split_freq-combined-forecast-GARW.tsv\n",
    "# cd projects/blab/rt-from-frequency-dynamics/estimates/omicron-countries-split\n",
    "# git log -- omicron-countries-split_freq-combined-forecast-GARW.tsv | grep commit | cut -d ' ' -f 2\n",
    "\n",
    "commits = [\n",
    "    'b7ff151ae7f54d79da2039304821e3c5a4aecdeb',\n",
    "    '4fec4d2c039be75b96aab5f2f89cfbf7ab1482cd',\n",
    "    'd4adf9740209ed0e8c1264dcc12514f27e09a890',\n",
    "    'a9ebb8ad8a6e83b0e0384d268cf5e98a41d8298f',\n",
    "    'd0c553297835d0741dba730c99a1a6de33d6f063',\n",
    "    '295330b9eeeab77bdfd39c2eb12b1e296ccb6b9f',\n",
    "    'd294df9bf4e2316015b39b33a15e8da576fdce81',\n",
    "    'cb3e4212b87ba717269055e0fc700b90426fbf4d',\n",
    "    'e65d2bda5335710c692d8a4db0333eebfc8e4e7a',\n",
    "    '0c5326fa316cba453e535079fdae10c98166c9d0',\n",
    "    '6b61079c537c2481e134364488ca34778b2b0aae',\n",
    "    '0c5062e918920ac9b49d4171dd575f8a7747baf8',\n",
    "    'bb7d8cd73bec104cebeb5f6b4fb26e790f9c5056',\n",
    "    '5ef08b50ee1b7c10895da8353a4155c5f88d977d',\n",
    "    '83a50c388e3ee92c17813bf9f4179a0a2d750f86',\n",
    "    'da809352020660b4c58be4bdc459ba09e20bea52',\n",
    "    '871188d411049ce238366c50b48e807a42690650',\n",
    "    '22626a6ceea4ca82be01be085c05f171cabfad51',\n",
    "    '759006497f536bfe957ba224e33fa9f8523c58fe',\n",
    "    '8333654e333cc3c3ec7b88a38901be52da1bf2a9',\n",
    "    '04a24bf70dafddee9ace4c8aec54f86a48dcbcac',\n",
    "    '8eb83dd04c51e674ec4a3a64faacfa3882f37dc4',\n",
    "]\n",
    "\n",
    "models = {}\n",
    "forecasts = {}\n",
    "for idx,c in enumerate(commits):\n",
    "    print(f\"{idx+1}/{len(commits)}. {c[0:7]}\")\n",
    "    github_url = f\"https://raw.githubusercontent.com/blab/rt-from-frequency-dynamics/{c}/estimates/omicron-countries-split/\"\n",
    "    forecasts[c[0:7]] = pd.read_table(github_url+\"omicron-countries-split_freq-combined-forecast-GARW.tsv\", sep=\"\\t\")\n",
    "    models[c[0:7]] = pd.read_table(github_url+\"omicron-countries-split_freq-combined-GARW.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08265754-1f26-4e33-abc9-93e6254ae5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799be5a1-7dd4-43f4-ae77-9c0a090ecd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(location, variant):\n",
    "    \"\"\"\n",
    "    Subset the dataframes and merge them together to represent the desired location + variant.\n",
    "    Expects the objects `forecasts`, `seq_counts` and `model_latest` to be in the namespace.\n",
    "    Returns a tuple of (first_day, df) where first_day is the date where the df has freq>0.1%\n",
    "    (the df is not subsetted)\n",
    "    \"\"\"\n",
    "\n",
    "    def curate_df(df, suffix, isModel):\n",
    "        df = df[(df[\"location\"]==location) & (df[\"variant\"]==variant)]\n",
    "\n",
    "        prefix = \"model\" if isModel else \"forecast\"\n",
    "        \n",
    "        # this can happen if (e.g.) the location wasn't in this model run,\n",
    "        # or the model run was before we defined a variant etc\n",
    "        if df.shape[0]==0:\n",
    "            return False\n",
    "\n",
    "        ## then pull out the columns we care about\n",
    "        ## NOTE. At some point the keys changed to include _forecast_, e.g.\n",
    "        ## old format: 'freq_upper_80' new format: 'freq_forecast_upper_80'\n",
    "        substr = \"\"\n",
    "        if any(['_forecast_' in name for name in df.columns]):\n",
    "            substr = '_forecast'\n",
    "            \n",
    "        df = df[[\"date\", f\"median_freq{substr}\",\n",
    "                 f\"freq{substr}_upper_50\", f\"freq{substr}_lower_50\", \n",
    "                 f\"freq{substr}_upper_80\", f\"freq{substr}_lower_80\", \n",
    "                 f\"freq{substr}_upper_95\", f\"freq{substr}_lower_95\"]]\n",
    "        # rename columns to avoid any merge messiness & be able to track things later on\n",
    "        df.rename(columns={\n",
    "            f'median_freq{substr}': f'{prefix}_median_{suffix}',\n",
    "            f'freq{substr}_lower_50': f'{prefix}_lower50_{suffix}',\n",
    "            f'freq{substr}_upper_50': f'{prefix}_upper50_{suffix}',\n",
    "            f'freq{substr}_lower_80': f'{prefix}_lower80_{suffix}',\n",
    "            f'freq{substr}_upper_80': f'{prefix}_upper80_{suffix}',\n",
    "            f'freq{substr}_lower_95': f'{prefix}_lower95_{suffix}',\n",
    "            f'freq{substr}_upper_95': f'{prefix}_upper95_{suffix}',\n",
    "        }, inplace=True)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    \n",
    "    # the latest model data fitted to actual (retrospective) data. (Update: not quite true. Todo.)\n",
    "    ## this is going to provide us the `median_freq` column...\n",
    "    subset_retrospective = model_latest[(model_latest[\"location\"]==location) & (model_latest[\"variant\"]==variant)]\n",
    "    subset_retrospective = subset_retrospective[[\"date\", \"median_freq\"]]\n",
    "\n",
    "    # the raw frequencies - this is going to give us the column `raw_freq`\n",
    "    ## modify seq_counts into something we can actually use...\n",
    "    raw = seq_counts[seq_counts[\"location\"]==location]\n",
    "    ## NOTE that the dates here have missing values, but this is ok as we'll merge into the model data which doesn't\n",
    "    raw = raw.pivot(index='date', columns='variant', values='sequences')\n",
    "    raw = raw.assign(total=raw.sum(axis=1))\n",
    "    raw.reset_index(inplace=True) ## to restore 'date' column\n",
    "    raw['raw_freq'] = raw.apply(lambda row: row[variant]/row['total'], axis=1)\n",
    "    raw = raw[[\"date\", \"raw_freq\"]]\n",
    "\n",
    "    # the models. This gives us a number of dataframes each with many columns, all starting with `model_`\n",
    "    subset_models = {k:curate_df(v, k, True) for k,v in models.items()}\n",
    "    subset_models = {k:v for k,v in subset_models.items() if v is not False}\n",
    "    \n",
    "    # the forecasts. This gives us a number of dataframes each with many columns, all starting with `forecast_`\n",
    "    subset_forecasts = {k:curate_df(v, k, False) for k,v in forecasts.items()}\n",
    "    subset_forecasts = {k:v for k,v in subset_forecasts.items() if v is not False}\n",
    "    \n",
    "    # merges all the above data frames\n",
    "    data = pd.merge(subset_retrospective, raw, how='outer', on='date')\n",
    "    for commit in subset_forecasts.keys():\n",
    "        data = pd.merge(data, subset_models[commit], how='outer', on='date')\n",
    "        data = pd.merge(data, subset_forecasts[commit], how='outer', on='date')\n",
    "\n",
    "    data.sort_values(by=['date'], ascending=True, inplace=True)    \n",
    "    summarise_dates(data) # useful check -- will print out warnings if dates aren't completely sequential\n",
    "\n",
    "    ## first date where freqs get above 0.1%\n",
    "    start_date = data[(data['median_freq']>0.001) | (data['raw_freq']>0.001)]['date'].iloc[0]\n",
    "\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # data = data[data.apply(lambda row: row['date']>=start_date, axis=1)]\n",
    "    # summarise_dates(data)\n",
    "    return (start_date, data)\n",
    "\n",
    "\n",
    "## Print out an example output to help:\n",
    "first_day, data = prepare_data(\"USA\", \"Omicron 22B\")\n",
    "print(first_day)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a27f4b-472f-4f15-9e57-a26a21e1e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colours_via_commit():\n",
    "    \"\"\"Associate commits to colours. Helps keep all plots in-sync\"\"\"\n",
    "    colours = {}\n",
    "    cmap = plt.get_cmap('viridis')(np.linspace(0.9, 0.25, len(commits)))\n",
    "    for idx, commit_long in enumerate(commits):\n",
    "        if idx==0:\n",
    "            colours[commit_long[0:7]] = [.66, .66, .66, 1] # final model (first commit) is grey\n",
    "        else:\n",
    "            colours[commit_long[0:7]] = cmap[idx]\n",
    "    return colours\n",
    "colours = colours_via_commit()\n",
    "\n",
    "def pangoise(clade):\n",
    "    if clade==\"Omicron 21L\": return \"BA.2\"\n",
    "    if clade==\"Omicron 22B\": return \"BA.5\"\n",
    "    return \"Unknown\" # Todo. Incomplete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd747e0b-170e-445d-922b-ca9894d945e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts(ax, data, title):\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    ## plot the \"truth\", as it's currently known -- i.e. based on the most recent \n",
    "    ## model run of all available data\n",
    "    ax.plot(data[\"date\"], data[\"median_freq\"], color='k', zorder=2)\n",
    "    ax.scatter(data[\"date\"], data[\"raw_freq\"], s=15, color='k', zorder=2)\n",
    "\n",
    "    ### which column names should we use for forecasts? (could be known a lot earlier)\n",
    "    forecast_names = [colname for colname in data.columns if colname.startswith('forecast_median_')]\n",
    "    model_names = [name.replace(\"forecast_\", \"model_\") for name in forecast_names]\n",
    "\n",
    "    for idx, colname in enumerate(forecast_names):\n",
    "        commit_hex = colname.split('_')[-1] ## short len=7 hex\n",
    "        c = colours[commit_hex]\n",
    "        if commit_hex == commits[0][0:7]: # first commit is the latest model (most recent model)\n",
    "            c = [.66, .66, .66, 1] # final model is grey\n",
    "\n",
    "        # c = colours[idx] if commits[0][0:7] not in colname else [.66, .66, .66, 1] # final model is grey\n",
    "        ax.plot(data[\"date\"], data[colname], color=c, zorder=4, lw=2)\n",
    "        colour_hls = colorsys.rgb_to_hls(*c[0:3])\n",
    "        lightened = colorsys.hls_to_rgb(colour_hls[0], 1 - 0.5 * (1 - colour_hls[1]), colour_hls[2])\n",
    "        # 95% CI first. ALl CIs below the real data (black lines / dots)\n",
    "        ax.fill_between(data[\"date\"], data[colname.replace('median', 'upper95')], data[colname.replace('median', 'lower95')],\n",
    "                        color=lightened, alpha = 0.2, lw=0, zorder=1)\n",
    "        # 80% CI \n",
    "        ax.fill_between(data[\"date\"], data[colname.replace('median', 'upper80')], data[colname.replace('median', 'lower80')],\n",
    "                        color=lightened, alpha = 0.2, lw=0, zorder=1)\n",
    "        # then 50% CI\n",
    "        ax.fill_between(data[\"date\"], data[colname.replace('median', 'upper50')], data[colname.replace('median', 'lower50')],\n",
    "                        color=lightened, alpha = 0.4, lw=0, zorder=1)\n",
    "\n",
    "    ### plot the model data (retrospective) which was available at the point in time the model was run.\n",
    "    ### this is the \"truth\" according to the data available at the time\n",
    "    for idx, colname in enumerate(model_names):\n",
    "        if commits[0][0:7] in colname:\n",
    "            continue ## don't plot the latest model run, it's already plotted in black...\n",
    "        commit_hex = colname.split('_')[-1] ## short len=7 hex\n",
    "        ax.plot(data[\"date\"], data[colname], color=colours[commit_hex], zorder=1, lw=2, dashes=[2, 2])\n",
    "        \n",
    "    #### x-labels\n",
    "    def nice_date(tick_idx):\n",
    "        d = date.fromisoformat(data['date'].iloc[tick_idx])\n",
    "        if tick_idx==0:\n",
    "            return d.strftime(\"%d %b %Y\")\n",
    "        if d.day==1:\n",
    "            return d.strftime(\"%d %b\")\n",
    "        return ''\n",
    "    \n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels([nice_date(x) for x in ax.get_xticks()])\n",
    "    ax.set_ylabel(\"Frequency\", size='large')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "## plot one just to see...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "first_day, data = prepare_data(\"USA\", \"Omicron 22B\")\n",
    "data = data[data.apply(lambda row: row['date']>=first_day, axis=1)]\n",
    "plot_forecasts(ax, data, \"BA.5 (22B) frequency in USA\")\n",
    "plt.show()\n",
    "\n",
    "del first_day, data, fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f37e15-8910-404d-b8d3-80963daac23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multiple(locations, variant):\n",
    "    \"\"\"\n",
    "    Helper fn to return a list of prepared datasets, one per location.\n",
    "    Dates before the first with any non-trivial frequencies are removed\n",
    "    \"\"\"\n",
    "    datasets = [prepare_data(location, variant) for location in locations]\n",
    "    first_day = sorted([d[0] for d in datasets])[0] ## different for each column, but consistent within a column\n",
    "    print(f\"Earliest day for {variant} across {len(locations)} locations: {first_day}\")\n",
    "    ## remove data before the first day when any location had non-trivial frequencies\n",
    "    datasets = [d[1][d[1].apply(lambda row: row['date']>=first_day, axis=1)] for d in datasets]\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c6a2-1f25-4466-89bb-8737dccb05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot with rows: various countries, and columns variants\n",
    "\n",
    "locations = [\"USA\", \"United Kingdom\", \"New Zealand\"]\n",
    "variants = [\"Omicron 21L\", \"Omicron 22B\"]\n",
    "fig, axes = plt.subplots(ncols=len(variants), nrows=len(locations), figsize=(12*len(variants), 6*len(locations)))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for cidx, variant in enumerate(variants):\n",
    "    datasets = prepare_multiple(locations, variant)\n",
    "    for ridx, location in enumerate(locations):\n",
    "        plot_forecasts(axes[ridx][cidx], datasets[ridx], f\"{variant} ({pangoise(variant)}) frequency in {location}\") \n",
    "    \n",
    "plt.savefig('forecast_evaluation.png', format=\"png\", bbox_inches='tight', transparent=False, pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "del datasets, axes, fig, location, locations, variant, variants, cidx, ridx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea44925-27db-4dc5-b482-d1e86d8a1169",
   "metadata": {},
   "source": [
    "## Measure the nowcast error from (later known) truth\n",
    "\n",
    "For each model estimate of past & current frequencies (i.e. not the forecast model), plot the difference between the estimate and the \"true\" frequency, where \"true\" is our current estimate with all available data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d775b2-1c77-4710-89ef-362da49f2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nowcast_error(data):\n",
    "\n",
    "    def extract_model_error(colname):\n",
    "        prefix = colname.split('_')[-1]\n",
    "        d = data[['date', 'median_freq', colname]]\n",
    "        d = d[d[colname].notnull()]\n",
    "        d[f'error_{prefix}'] = d[colname]-d['median_freq']\n",
    "        d['t'] = d.index - d.index[-1]\n",
    "        d = d[['t', f'error_{prefix}']]\n",
    "        return d\n",
    "    \n",
    "    model_names = [n for n in data.columns if n.startswith('model_median_')]\n",
    "    model_errors = [extract_model_error(colname) for colname in model_names]\n",
    "    df_merged = pd.merge(model_errors[0], model_errors[1], how='outer', on='t')\n",
    "    for df in model_errors[2:]:\n",
    "        df_merged = pd.merge(df_merged, df, how='outer', on='t')\n",
    "    # d1 = extract_model_error('model_median_b7ff151')\n",
    "    # d2 = extract_model_error('model_median_4fec4d2')\n",
    "    # df_merged = pd.merge(d1, d2, how='outer', on='t')\n",
    "    df_merged.sort_values(by=['t'], ascending=True, inplace=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "## Show an example df:\n",
    "collect_nowcast_error(prepare_data(\"USA\", \"Omicron 22B\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796b904-31bd-49f2-8432-41c0e2bcee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nowcast_error(ax, data, title, ymin=False, ymax=False):\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    for colname in data.columns:\n",
    "        if colname == 't':\n",
    "            continue\n",
    "        commit_short = colname.split('_')[-1]\n",
    "        c = colours[commit_short]\n",
    "        ax.plot(data[\"t\"], data[colname], color=c, zorder=2)\n",
    "\n",
    "    ## construct x-ticks to be weeks:\n",
    "    xtickvals = [0]\n",
    "    while xtickvals[-1]>data['t'].iloc[0]:\n",
    "        xtickvals.append(xtickvals[-1]-7)\n",
    "    ax.set_xticks(xtickvals)\n",
    "    ax.set_xticklabels([int(x/7) for x in xtickvals])\n",
    "    ax.set_xlabel('Weeks prior to final day in model output', size='large')\n",
    "    \n",
    "    # y-ticks:\n",
    "    if ymax is False:\n",
    "        ymax = max(data.max().drop('t'))\n",
    "    if ymin is False:\n",
    "        ymin = min(data.min().drop('t'))\n",
    "    yticks = np.linspace(-1, 1, 21) # -1, -0.9, ..., 0.9, 1\n",
    "    yticks = [y for y in yticks if y<=ymax and y>=ymin]\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels([f'{round(y*100)}%' for y in yticks])\n",
    "    ax.set_ylabel(\"âˆ†frequency (from eventual truth)\", size='large')\n",
    "    \n",
    "    for y in yticks:\n",
    "        if round(y,1)==0:\n",
    "            ax.axhline(y=0, c='k', zorder=1)\n",
    "        else:\n",
    "            ax.axhline(y=y, c='k', alpha=0.1, zorder=1, dashes=[2,6])\n",
    "\n",
    "    ax.set_title(title, size='x-large')\n",
    "\n",
    "\n",
    "## plot one just to see...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df = collect_nowcast_error(prepare_data(\"USA\", \"Omicron 21L\")[1]) ## subset prepare data based on 1st day?\n",
    "plot_nowcast_error(ax, df, \"Nowcast error (USA, 21L)\")\n",
    "plt.show()\n",
    "\n",
    "del fig, ax, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cc628-2579-4755-85b6-3f4f23bb62a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf45a-d45f-4c15-b2f0-418fe909c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"USA\", \"United Kingdom\", \"New Zealand\"]\n",
    "variant = \"Omicron 21L\"\n",
    "datasets = prepare_multiple(locations, variant)\n",
    "errors = [collect_nowcast_error(df) for df in datasets]\n",
    "ymin = min([min(df.min().drop('t')) for df in errors])\n",
    "ymax = max([max(df.max().drop('t')) for df in errors])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(locations), figsize=(10, 5*len(locations)))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    location = locations[idx]\n",
    "    plot_nowcast_error(ax, errors[idx], f\"Nowcast error ({location}, {variant})\", ymin=ymin, ymax=ymax)\n",
    "                     \n",
    "plt.savefig(f'nowcast_error.{variant.split()[-1]}.png', format=\"png\", bbox_inches='tight', transparent=False, pad_inches=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del datasets, idx, ax, fig, axes, location, locations, variant, errors, ymin, ymax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a89e0-cd73-4f75-9726-c5c99d693aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
